# Visual Training Project in 2025 Chinese College Students' Engineering Innovation Competition (Waste Sorting Track)

## Project Overview
This project is developed for the visual training segment of the 2025 Chinese College Students' Engineering Innovation Competition (Waste Sorting Track). It integrates the YOLOv5 training framework, testing code, a waste sorting dataset (in YOLO format with images), and pre-trained models.
### Preface
In the preliminary round of this competition, ten types of waste were clearly specified. These wastes have significant similarities in shape and color, and there are also many small objects, which undoubtedly pose great challenges to image recognition and localization. To effectively address such issues, we employed the YOLOv5 object detection algorithm for image recognition and localization tasks.

YOLOv5 has unique advantages. It adopts an end - to - end network structure and ingeniously transforms the object detection task into a regression problem. This transformation enables the algorithm to directly detect and classify objects. Its working principle is to apply a Convolutional Neural Network (CNN) to the entire image, carefully dividing the image into several grids, and then predicting the class probability and bounding box of each grid.

In practical applications, YOLOv5 has demonstrated high accuracy and practicality. It can accurately recognize these ten types of waste with similar features and containing small objects in complex waste image scenes, and precisely locate their positions in the images, providing reliable data support for subsequent waste - handling related work. 

 
### DataBase
Based on the external characteristics and internal properties of waste, we first conduct a detailed classification of waste and then categorize it into four major types of waste. Specifically, we initially divide waste into the following categories: stones (including bricks), ceramic tiles, batteries, pills (together with medicine boxes), recyclable items (specifically referring to bottles or cups with a volume of less than 100 milliliters), and common vegetables such as white radishes, red radishes, and potatoes.

## Directory Structure and File Descriptions

### Files
1. **mydata**
    - **Type**: Folder
    - **Function**: Stores custom data related to this project, potentially including specific waste sorting data, annotation files, etc., for training and testing purposes.
2. **yolov5s.pt**
    - **Type**: PT File
    - **Function**: Pretrained YOLOv5s weight file. It contains model parameters pre-trained on a large-scale dataset, serving as a starting point for training in this project to accelerate convergence and improve training efficiency.
3. **export.py**
    - **Type**: PY File
    - **Function**: Used to export trained models into different formats for deployment in various environments (e.g., mobile devices, embedded systems).
4. **train.py**
    - **Type**: PY File
    - **Function**: YOLOv5 training script. Configures relevant parameters to train the model using the waste sorting dataset, adjusting model weights to optimize performance on waste classification tasks.
5. **val.py**
    - **Type**: PY File
    - **Function**: Validates and evaluates the model during training or after training. Computes metrics such as mAP (mean Average Precision) to measure the model's performance on waste sorting tasks.
6. **detect.py**
    - **Type**: PY File
    - **Function**: Detection script that uses trained models to perform object detection on input images or videos, identifying waste categories. It can be applied to real-time detection in practical scenarios.
7. **hubconf.py**
    - **Type**: PY File
    - **Function**: Defines configuration information for the model in PyTorch Hub, facilitating quick model loading and usage through PyTorch Hub.
8. **benchmarks.py**
    - **Type**: PY File
    - **Function**: Conducts performance benchmark tests on the model, such as calculating inference speed and memory usage, to evaluate the model's operational efficiency in different hardware environments.

### Folders
1. **.idea**
    - **Type**: Folder
    - **Function**: Stores project configuration information for development tools like IntelliJ IDEA, including project structure, run configurations, and debugging settings.
2. **__pycache__**
    - **Type**: Folder
    - **Function**: Contains cached bytecode files generated by the Python interpreter during runtime to improve Python program loading speed. These files are automatically generated during Python execution.
3. **classify**
    - **Type**: Folder
    - **Function**: May store code, configuration files, or auxiliary tools related to waste category classification, such as feature extraction for waste categories and training of classification models.
4. **data**
    - **Type**: Folder
    - **Function**: Stores data files required for the project, such as configuration files for the waste sorting dataset, data augmentation parameter settings, and metadata information about the dataset.
5. **models**
    - **Type**: Folder
    - **Function**: Contains definition files for the YOLOv5 model, including network structure definitions and configuration files for different model versions. It is crucial for constructing and modifying the YOLOv5 model architecture.
6. **runs**
    - **Type**: Folder
    - **Function**: Stores result files generated during training and testing, such as log files, saved model weights, and visualization files of test results.
7. **segment**
    - **Type**: Folder
    - **Function**: If the project involves instance segmentation tasks for waste sorting, this folder may store code, models, and configuration files related to segmentation for precise segmentation and recognition of waste targets.
8. **utils**
    - **Type**: Folder
    - **Function**: Stores various utility functions and auxiliary scripts used in the project, such as data preprocessing functions, model evaluation metric calculation functions, and visualization tools, to support the overall operation and development of the project.
9. **venv**
    - **Type**: Folder
    - **Function**: Python virtual environment folder used to isolate the project's runtime environment, ensuring that Python packages and versions required by the project are separate from the system environment and avoiding dependency conflicts between different projects.

## Usage Instructions
1. **Prepare the Dataset**: Ensure the waste sorting dataset (in YOLO format with images) in the `mydata` folder is complete and correctly annotated.
2. **Configure Parameters**: Set relevant parameters in scripts like `train.py` and `val.py`, such as training epochs, learning rate, and dataset paths.
3. **Train the Model**: Run the `train.py` script to start model training. Training results will be saved in the `runs` folder.
4. **Validate and Test**: Use `val.py` to validate and evaluate the model, and `detect.py` to perform waste classification detection on new images or videos.
5. **Export the Model**: If deployment in other environments is needed, run `export.py` to export the trained model into a suitable format.

## Notes
1. Ensure the Python environment and relevant dependencies are correctly installed. Refer to the official YOLOv5 documentation for installation and configuration guidance.
2. Thoroughly test the model's performance and adjust parameters as needed before applying the model to practical applications.
## Training slice:
To evaluate the adaptability of the model, we conducted tests on an unseen validation dataset. The following images show some slices from the validation process. As can be seen, the recognition results generally meet the requirements, with detection boxes closely fitting the target objects. For targets like batteries and pills, the bounding boxes accurately locate the objects, demonstrating the effectiveness of YOLOv5 in object localization tasks and meeting the needs for image positioning.
However, the confidence scores for some waste categories did not reach the expected level, and there was significant fluctuation in the confidence levels of certain classes. For example, the confidence score for "yaopian (pills)" was as low as 0.4 in some annotations, indicating that the model's feature extraction for such targets (e.g., pills/medicine boxes with complex shapes and colors) lacks stability. 
To address this issue, future improvements could include enriching the training dataset (e.g., adding samples of pills from different angles and occlusion scenarios) and adjusting model hyperparameters (e.g., optimizing the loss function and learning rate strategy) to enhance the model's robustness in recognizing objects with complex morphologies.

![image](https://github.com/user-attachments/assets/2ce91204-6009-463d-aadf-542a82f9854b)
![image](https://github.com/user-attachments/assets/de9df83b-78fd-436c-8d27-964cba7f86aa)
![image](https://github.com/user-attachments/assets/9f037c1c-454e-44e0-b980-691043503da6)
![image](https://github.com/user-attachments/assets/56cce496-7f9f-4b3a-b5e9-a3b5abb2185e)

## Traning Idea:
### CBAM Attention Mechanism
The Convolutional Block Attention Module (CBAM) is an attention mechanism designed for Convolutional Neural Networks (CNNs). It enhances the network's ability to focus on critical input features, thereby improving performance. CBAM consists of two sub-modules:

1. **Channel Attention Module (CAM)**  
   - Focuses on "what" is meaningful by computing a channel-wise attention map.  
   - Uses global average pooling (GAP) and global max pooling (GMP) to aggregate spatial information.  
   - A multi-layer perceptron (MLP) with a shared hidden layer then processes these pooled features to generate channel weights.  
   - These weights are applied to the original feature map via element-wise multiplication, emphasizing important channels.

2. **Spatial Attention Module (SAM)**  
   - Focuses on "where" important features are located by generating a spatial attention map.  
   - Computes average and max values across channels to capture spatial cues.  
   - A convolutional layer processes these pooled features to generate a spatial attention map.  
   - This map is applied to the feature map to enhance regions containing critical information.
   
![image](https://github.com/user-attachments/assets/8abf8922-bfee-4165-a7a5-4edc6410f429)


This approach aligns with our goal of maximizing YOLOv5's efficiency while maintaining its compact size.

After integrating the YOLOv5 backbone network with the CBAM attention mechanism, we compared the changes in various parameters and plotted relevant graphs. Based on the execution results, line charts have been successfully drawn to compare the trends of each parameter with the number of training epochs when the attention mechanism is used and when it is not. From these trends, the following conclusions can be drawn: 

![image](https://github.com/user-attachments/assets/afb7ca6b-41ed-471a-9d63-effeedc559d0)

### the Asymptotic Feature Pyramid Network（AFPN）
To further improve the recognition accuracy (mAP) of YOLOv5s, we modified the backbone network of YOLOv5s and incorporated a network structure designed for detecting small objects, thereby achieving a further boost in accuracy.

As previously mentioned, we've already introduced the CBAM attention mechanism. Now, we propose the Asymptotic Feature Pyramid Network (AFPN), a network structure for multi-scale feature fusion in object detection。

### SPPCSPC
SPPCSPC is a feature processing module in object detection models such as YOLOv7, integrating the advantages of Spatial Pyramid Pooling (SPP) and Cross Stage Partial Connection (CSP).

SPPCSPC achieves the effective extraction and fusion of multi-scale features by adding parallel multiple MaxPool operations in a series of convolutions and combining the structure of CSPN. The SPP layer can convert feature maps of any size into fixed - size feature vectors, effectively avoiding image distortion problems caused by operations such as image cropping and scaling, and solving the problem of repeated feature extraction of images in Convolutional Neural Networks. The CSPN structure reduces the computational load by splitting and reusing feature maps while maintaining the model's representational ability. By combining the advantages of SPP and CSPN, the SPPCSPC module effectively enhances the model's multi-scale feature representation ability and computational efficiency, demonstrating excellent performance in computer vision tasks such as object detection. However, in practical applications, attention should be paid to issues such as parameter tuning complexity and computational overhead. 

### Compare Different Modles
Now, we will compare the mAP (Mean Average Precision) trends of three models through visualization and conduct an optimal analysis.

![image](https://github.com/user-attachments/assets/c07d9987-eade-4368-a1d6-e9247f6ccca2)

|modles|CBAM|SPPCSPC|AFPN|
|--|--|--|--|
|MAX(MAP(0.5 - 0.95))|0.78371|0.78918|0.77229|
|MAX(MAP(0.5))|0.995|0.99495|0.9512|

Based on the analysis of the above chart, the following conclusions can be drawn:
① CBAM achieved the highest value of 0.995 in the single-threshold mAP@0.5, slightly higher than the 0.99495 of SPPCSPC.
② SPPCSPC performed best in the comprehensive threshold mAP@0.5 - 0.95, reaching 0.78918.
③ AFPN was significantly lower than the other models in both indicators.

Integrate different combinations of multiple models and conduct combined training on the same dataset. Evaluate the models by observing their mAP values: 

|modles|MAP(0.5)|MAP(0.5 - 0.9)|
|--|--|--|
|CBAM+SPPCSPC|0.995|0.79171|
|CBAM+SPPCSPC+AFPN|0.995|0.78503|
|CBAM+AFPN|0.995|0.77681|

Under the metrics/mAP_0.5:0.95 indicator, the CBAM+SPPCSPC model has the highest optimal value. This means that this model has relatively better comprehensive detection performance within a wider IoU range (from 0.5 to 0.95).

Overall, if more emphasis is placed on the metrics/mAP_0.5 indicator, the three models perform similarly. However, if more importance is attached to the metrics/mAP_0.5:0.95 indicator, the CBAM+SPPCSPC model performs optimally.

In summary, the MAP(0.5 - 0.9) of the CBAM+SPPCSPC is the best among all models. Followed by SPPCSPC and CBAM+SPPCSPC+AFPN. The performance of CBAM and AFPN is not as promising compared to other types.

